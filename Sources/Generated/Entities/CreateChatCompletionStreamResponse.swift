// Generated by Create API
// https://github.com/CreateAPI/CreateAPI

import Foundation

/// Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
public struct CreateChatCompletionStreamResponse: Codable {
    /// A unique identifier for the chat completion. Each chunk has the same ID.
    public var id: String
    /// A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the
    /// last chunk if you set `stream_options: {"include_usage": true}`.
    public var choices: [Choice]
    /// The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
    public var created: Int
    /// The model to generate the completion.
    public var model: String
    /// The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
    ///
    /// Example: "scale"
    public var serviceTier: ServiceTier?
    /// This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
    public var systemFingerprint: String?
    /// The object type, which is always `chat.completion.chunk`.
    public var object: Object
    /// An optional field that will only be present when you set `stream_options: {"include_usage": true}` in your request.
    /// When present, it contains a null value except for the last chunk which contains the token usage statistics for the entire request.
    public var usage: Usage?

    public struct Choice: Codable {
        /// A chat completion delta generated by streamed model responses.
        public var delta: ChatCompletionStreamResponseDelta
        /// Log probability information for the choice.
        public var logprobs: Logprobs?
        /// The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
        /// `length` if the maximum number of tokens specified in the request was reached,
        /// `content_filter` if content was omitted due to a flag from our content filters,
        /// `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
        public var finishReason: FinishReason?
        /// The index of the choice in the list of choices.
        public var index: Int

        /// Log probability information for the choice.
        public struct Logprobs: Codable {
            /// A list of message content tokens with log probability information.
            public var content: [ChatCompletionTokenLogprob]?
            /// A list of message refusal tokens with log probability information.
            public var refusal: [ChatCompletionTokenLogprob]?

            public init(content: [ChatCompletionTokenLogprob]? = nil, refusal: [ChatCompletionTokenLogprob]? = nil) {
                self.content = content
                self.refusal = refusal
            }

            public init(from decoder: Decoder) throws {
                let values = try decoder.container(keyedBy: StringCodingKey.self)
                self.content = try values.decodeIfPresent([ChatCompletionTokenLogprob].self, forKey: "content")
                self.refusal = try values.decodeIfPresent([ChatCompletionTokenLogprob].self, forKey: "refusal")
            }

            public func encode(to encoder: Encoder) throws {
                var values = encoder.container(keyedBy: StringCodingKey.self)
                try values.encodeIfPresent(content, forKey: "content")
                try values.encodeIfPresent(refusal, forKey: "refusal")
            }
        }

        /// The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
        /// `length` if the maximum number of tokens specified in the request was reached,
        /// `content_filter` if content was omitted due to a flag from our content filters,
        /// `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
        public enum FinishReason: String, Codable, CaseIterable {
            case stop
            case length
            case toolCalls = "tool_calls"
            case contentFilter = "content_filter"
            case functionCall = "function_call"
        }

        public init(delta: ChatCompletionStreamResponseDelta, logprobs: Logprobs? = nil, finishReason: FinishReason? = nil, index: Int) {
            self.delta = delta
            self.logprobs = logprobs
            self.finishReason = finishReason
            self.index = index
        }

        public init(from decoder: Decoder) throws {
            let values = try decoder.container(keyedBy: StringCodingKey.self)
            self.delta = try values.decode(ChatCompletionStreamResponseDelta.self, forKey: "delta")
            self.logprobs = try values.decodeIfPresent(Logprobs.self, forKey: "logprobs")
            self.finishReason = try values.decodeIfPresent(FinishReason.self, forKey: "finish_reason")
            self.index = try values.decode(Int.self, forKey: "index")
        }

        public func encode(to encoder: Encoder) throws {
            var values = encoder.container(keyedBy: StringCodingKey.self)
            try values.encode(delta, forKey: "delta")
            try values.encodeIfPresent(logprobs, forKey: "logprobs")
            try values.encodeIfPresent(finishReason, forKey: "finish_reason")
            try values.encode(index, forKey: "index")
        }
    }

    /// The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
    ///
    /// Example: "scale"
    public enum ServiceTier: String, Codable, CaseIterable {
        case scale
        case `default`
    }

    /// The object type, which is always `chat.completion.chunk`.
    public enum Object: String, Codable, CaseIterable {
        case chatCompletionChunk = "chat.completion.chunk"
    }

    /// An optional field that will only be present when you set `stream_options: {"include_usage": true}` in your request.
    /// When present, it contains a null value except for the last chunk which contains the token usage statistics for the entire request.
    public struct Usage: Codable {
        /// Number of tokens in the generated completion.
        public var completionTokens: Int
        /// Number of tokens in the prompt.
        public var promptTokens: Int
        /// Total number of tokens used in the request (prompt + completion).
        public var totalTokens: Int

        public init(completionTokens: Int, promptTokens: Int, totalTokens: Int) {
            self.completionTokens = completionTokens
            self.promptTokens = promptTokens
            self.totalTokens = totalTokens
        }

        public init(from decoder: Decoder) throws {
            let values = try decoder.container(keyedBy: StringCodingKey.self)
            self.completionTokens = try values.decode(Int.self, forKey: "completion_tokens")
            self.promptTokens = try values.decode(Int.self, forKey: "prompt_tokens")
            self.totalTokens = try values.decode(Int.self, forKey: "total_tokens")
        }

        public func encode(to encoder: Encoder) throws {
            var values = encoder.container(keyedBy: StringCodingKey.self)
            try values.encode(completionTokens, forKey: "completion_tokens")
            try values.encode(promptTokens, forKey: "prompt_tokens")
            try values.encode(totalTokens, forKey: "total_tokens")
        }
    }

    public init(id: String, choices: [Choice], created: Int, model: String, serviceTier: ServiceTier? = nil, systemFingerprint: String? = nil, object: Object, usage: Usage? = nil) {
        self.id = id
        self.choices = choices
        self.created = created
        self.model = model
        self.serviceTier = serviceTier
        self.systemFingerprint = systemFingerprint
        self.object = object
        self.usage = usage
    }

    public init(from decoder: Decoder) throws {
        let values = try decoder.container(keyedBy: StringCodingKey.self)
        self.id = try values.decode(String.self, forKey: "id")
        self.choices = try values.decode([Choice].self, forKey: "choices")
        self.created = try values.decode(Int.self, forKey: "created")
        self.model = try values.decode(String.self, forKey: "model")
        self.serviceTier = try values.decodeIfPresent(ServiceTier.self, forKey: "service_tier")
        self.systemFingerprint = try values.decodeIfPresent(String.self, forKey: "system_fingerprint")
        self.object = try values.decode(Object.self, forKey: "object")
        self.usage = try values.decodeIfPresent(Usage.self, forKey: "usage")
    }

    public func encode(to encoder: Encoder) throws {
        var values = encoder.container(keyedBy: StringCodingKey.self)
        try values.encode(id, forKey: "id")
        try values.encode(choices, forKey: "choices")
        try values.encode(created, forKey: "created")
        try values.encode(model, forKey: "model")
        try values.encodeIfPresent(serviceTier, forKey: "service_tier")
        try values.encodeIfPresent(systemFingerprint, forKey: "system_fingerprint")
        try values.encode(object, forKey: "object")
        try values.encodeIfPresent(usage, forKey: "usage")
    }
}
