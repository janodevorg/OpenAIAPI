// Generated by Create API
// https://github.com/CreateAPI/CreateAPI

import Foundation

public struct CreateFineTuneRequest: Codable {
    /// The ID of an uploaded file that contains training data.
    /// 
    /// See [upload file](/docs/api-reference/files/upload) for how to upload a file.
    /// 
    /// Your dataset must be formatted as a JSONL file, where each training
    /// example is a JSON object with the keys "prompt" and "completion".
    /// Additionally, you must upload your file with the purpose `fine-tune`.
    /// 
    /// See the [fine-tuning guide](/docs/guides/fine-tuning/creating-training-data) for more details.
    ///
    /// Example: "file-ajSREls59WBbvgSzJSVWxMCB"
    public var trainingFile: String
    /// The ID of an uploaded file that contains validation data.
    /// 
    /// If you provide this file, the data is used to generate validation
    /// metrics periodically during fine-tuning. These metrics can be viewed in
    /// the [fine-tuning results file](/docs/guides/fine-tuning/analyzing-your-fine-tuned-model).
    /// Your train and validation data should be mutually exclusive.
    /// 
    /// Your dataset must be formatted as a JSONL file, where each validation
    /// example is a JSON object with the keys "prompt" and "completion".
    /// Additionally, you must upload your file with the purpose `fine-tune`.
    /// 
    /// See the [fine-tuning guide](/docs/guides/fine-tuning/creating-training-data) for more details.
    ///
    /// Example: "file-XjSREls59WBbvgSzJSVWxMCa"
    public var validationFile: String?
    /// The name of the base model to fine-tune. You can select one of "ada",
    /// "babbage", "curie", "davinci", or a fine-tuned model created after 2022-04-21.
    /// To learn more about these models, see the
    /// [Models](https://platform.openai.com/docs/models) documentation.
    public var model: String?
    /// The number of epochs to train the model for. An epoch refers to one
    /// full cycle through the training dataset.
    public var nEpochs: Int?
    /// The batch size to use for training. The batch size is the number of
    /// training examples used to train a single forward and backward pass.
    /// 
    /// By default, the batch size will be dynamically configured to be
    /// ~0.2% of the number of examples in the training set, capped at 256 -
    /// in general, we've found that larger batch sizes tend to work better
    /// for larger datasets.
    public var batchSize: Int?
    /// The learning rate multiplier to use for training.
    /// The fine-tuning learning rate is the original learning rate used for
    /// pretraining multiplied by this value.
    /// 
    /// By default, the learning rate multiplier is the 0.05, 0.1, or 0.2
    /// depending on final `batch_size` (larger learning rates tend to
    /// perform better with larger batch sizes). We recommend experimenting
    /// with values in the range 0.02 to 0.2 to see what produces the best
    /// results.
    public var learningRateMultiplier: Double?
    /// The weight to use for loss on the prompt tokens. This controls how
    /// much the model tries to learn to generate the prompt (as compared
    /// to the completion which always has a weight of 1.0), and can add
    /// a stabilizing effect to training when completions are short.
    /// 
    /// If prompts are extremely long (relative to completions), it may make
    /// sense to reduce this weight so as to avoid over-prioritizing
    /// learning the prompt.
    public var promptLossWeight: Double?
    /// If set, we calculate classification-specific metrics such as accuracy
    /// and F-1 score using the validation set at the end of every epoch.
    /// These metrics can be viewed in the [results file](/docs/guides/fine-tuning/analyzing-your-fine-tuned-model).
    /// 
    /// In order to compute classification metrics, you must provide a
    /// `validation_file`. Additionally, you must
    /// specify `classification_n_classes` for multiclass classification or
    /// `classification_positive_class` for binary classification.
    public var isComputeClassificationMetrics: Bool
    /// The number of classes in a classification task.
    /// 
    /// This parameter is required for multiclass classification.
    public var classificationNClasses: Int?
    /// The positive class in binary classification.
    /// 
    /// This parameter is needed to generate precision, recall, and F1
    /// metrics when doing binary classification.
    public var classificationPositiveClass: String?
    /// If this is provided, we calculate F-beta scores at the specified
    /// beta values. The F-beta score is a generalization of F-1 score.
    /// This is only used for binary classification.
    /// 
    /// With a beta of 1 (i.e. the F-1 score), precision and recall are
    /// given the same weight. A larger beta score puts more weight on
    /// recall and less on precision. A smaller beta score puts more weight
    /// on precision and less on recall.
    ///
    /// Example:
    ///
    /// [
    ///   0.59999999999999998,
    ///   1,
    ///   1.5,
    ///   2
    /// ]
    public var classificationBetas: [Double]?
    /// A string of up to 40 characters that will be added to your fine-tuned model name.
    /// 
    /// For example, a `suffix` of "custom-model-name" would produce a model name like `ada:ft-your-org:custom-model-name-2022-02-15-04-21-04`.
    public var suffix: String?

    public init(trainingFile: String, validationFile: String? = nil, model: String? = nil, nEpochs: Int? = nil, batchSize: Int? = nil, learningRateMultiplier: Double? = nil, promptLossWeight: Double? = nil, isComputeClassificationMetrics: Bool? = nil, classificationNClasses: Int? = nil, classificationPositiveClass: String? = nil, classificationBetas: [Double]? = nil, suffix: String? = nil) {
        self.trainingFile = trainingFile
        self.validationFile = validationFile
        self.model = model
        self.nEpochs = nEpochs
        self.batchSize = batchSize
        self.learningRateMultiplier = learningRateMultiplier
        self.promptLossWeight = promptLossWeight
        self.isComputeClassificationMetrics = isComputeClassificationMetrics ?? false
        self.classificationNClasses = classificationNClasses
        self.classificationPositiveClass = classificationPositiveClass
        self.classificationBetas = classificationBetas
        self.suffix = suffix
    }

    public init(from decoder: Decoder) throws {
        let values = try decoder.container(keyedBy: StringCodingKey.self)
        self.trainingFile = try values.decode(String.self, forKey: "training_file")
        self.validationFile = try values.decodeIfPresent(String.self, forKey: "validation_file")
        self.model = try values.decodeIfPresent(String.self, forKey: "model")
        self.nEpochs = try values.decodeIfPresent(Int.self, forKey: "n_epochs")
        self.batchSize = try values.decodeIfPresent(Int.self, forKey: "batch_size")
        self.learningRateMultiplier = try values.decodeIfPresent(Double.self, forKey: "learning_rate_multiplier")
        self.promptLossWeight = try values.decodeIfPresent(Double.self, forKey: "prompt_loss_weight")
        self.isComputeClassificationMetrics = try values.decodeIfPresent(Bool.self, forKey: "compute_classification_metrics") ?? false
        self.classificationNClasses = try values.decodeIfPresent(Int.self, forKey: "classification_n_classes")
        self.classificationPositiveClass = try values.decodeIfPresent(String.self, forKey: "classification_positive_class")
        self.classificationBetas = try values.decodeIfPresent([Double].self, forKey: "classification_betas")
        self.suffix = try values.decodeIfPresent(String.self, forKey: "suffix")
    }

    public func encode(to encoder: Encoder) throws {
        var values = encoder.container(keyedBy: StringCodingKey.self)
        try values.encode(trainingFile, forKey: "training_file")
        try values.encodeIfPresent(validationFile, forKey: "validation_file")
        try values.encodeIfPresent(model, forKey: "model")
        try values.encodeIfPresent(nEpochs, forKey: "n_epochs")
        try values.encodeIfPresent(batchSize, forKey: "batch_size")
        try values.encodeIfPresent(learningRateMultiplier, forKey: "learning_rate_multiplier")
        try values.encodeIfPresent(promptLossWeight, forKey: "prompt_loss_weight")
        try values.encodeIfPresent(isComputeClassificationMetrics, forKey: "compute_classification_metrics")
        try values.encodeIfPresent(classificationNClasses, forKey: "classification_n_classes")
        try values.encodeIfPresent(classificationPositiveClass, forKey: "classification_positive_class")
        try values.encodeIfPresent(classificationBetas, forKey: "classification_betas")
        try values.encodeIfPresent(suffix, forKey: "suffix")
    }
}
